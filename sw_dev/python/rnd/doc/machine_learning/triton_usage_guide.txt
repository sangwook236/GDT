[-] General.
	- Site.
		https://developer.nvidia.com/nvidia-triton-inference-server
		https://github.com/triton-inference-server/server
		https://github.com/triton-inference-server/client

	- Document.
		https://github.com/triton-inference-server/server/blob/main/README.md#documentation

		Metrics:
			https://github.com/triton-inference-server/server/blob/main/docs/metrics.md

[-] Usage.
	- Quickstart.
		https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md

		Create a model repository:
			cd ${TritonInferenceServer_HOME}/docs/examples
			./fetch_models.sh

		Run Triton:
			docker run --gpus all --rm nvcr.io/nvidia/tritonserver:<xx.yy>-py3 nvidia-smi
				For CPU-only system:
					<error> WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
			docker run --rm nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --help

			Run on system with GPUs:
				docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
					docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/sangwook/my_repo/cpp/triton_inference_server_github/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:21.07-py3 tritonserver --model-repository=/models
				docker run --gpus=1 --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models --http-port=8000 --grpc-port=8001 --metrics-port=8002 --log-verbose=1
			Run on CPU-only system:
				docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
				docker run --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models --http-port=8000 --grpc-port=8001 --metrics-port=8002 --log-verbose=1

		Verify Triton is running correctly:
			curl -v localhost:8000/v2/health/ready

		Get the client libraries:
			docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		Run the client image:
			docker run -it --rm --net=host nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		Run the simple examples:
			/workspace/install/bin/simple_http_infer_client
			/workspace/install/bin/simple_http_async_infer_client
			/workspace/install/bin/simple_http_shm_client
			/workspace/install/bin/simple_http_cudashm_client

			/workspace/install/bin/simple_grpc_infer_client
			/workspace/install/bin/simple_grpc_async_infer_client
			/workspace/install/bin/simple_grpc_shm_client
			/workspace/install/bin/simple_grpc_cudashm_client

			/workspace/install/bin/simple_http_string_infer_client
			/workspace/install/bin/simple_grpc_string_infer_client

			/workspace/install/bin/simple_http_sequence_sync_infer_client
			/workspace/install/bin/simple_grpc_sequence_stream_infer_client
			/workspace/install/bin/simple_grpc_sequence_sync_infer_client

			/workspace/install/bin/simple_http_model_control
			/workspace/install/bin/simple_grpc_model_control

			/workspace/install/bin/simple_http_health_metadata
			/workspace/install/bin/simple_grpc_health_metadata

		Run the image classification example:
			/workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION /workspace/images/mug.jpg
			/workspace/install/bin/image_client -m densenet_onnx -c 3 -s VGG /workspace/images/mug.jpg

			/workspace/install/bin/image_client -m inception_graphdef -c 3 -s INCEPTION /workspace/images/mug.jpg
			/workspace/install/bin/image_client -m inception_graphdef -c 3 -s VGG /workspace/images/mug.jpg

		Analyze performance:
			/workspace/install/bin/perf_analyzer -m simple
			/workspace/install/bin/perf_analyzer -m simple --concurrency-range 1:10
			/workspace/install/bin/perf_analyzer -m simple_string
			/workspace/install/bin/perf_analyzer -m simple_sequence
			/workspace/install/bin/perf_analyzer -m densenet_onnx

	- Deployment.
		https://github.com/triton-inference-server/server/tree/main/deploy

	- TorchScript.
		REF [directory] >> ./triton

		Prepare models:
			cd ./triton

			Train models:
				python cifar10_trainer.py

			For GPU:
				cp ./cifar10_gpu_ts.pth ./model_repository/pytorch_cifar10/1/model.pt
			For CPU:
				cp ./cifar10_cpu_ts.pth ./model_repository/pytorch_cifar10/1/model.pt

		Run Triton:
			Run on system with GPUs:
				docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
					docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/sangwook/work/SWDT_github/sw_dev/python/rnd/doc/machine_learning/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.07-py3 tritonserver --model-repository=/models

			Run on CPU-only system:
				docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
					docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/sangwook/work/SWDT_github/sw_dev/python/rnd/doc/machine_learning/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.07-py3 tritonserver --model-repository=/models

		Verify Triton is running correctly:
			curl -v localhost:8000/v2/health/ready

		Get the client libraries:
			docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		Run the client image:
			docker run -it --rm -v/full/path/to/model_repository:/workspace --net=host nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk
				docker run -it --rm -v/home/sangwook/work/SWDT_github/sw_dev/python/rnd/doc/machine_learning/triton/cifar10_images:/workspace/cifar10_images --net=host nvcr.io/nvidia/tritonserver:21.07-py3-sdk

			Run the image classification example:
				/workspace/install/bin/image_client -m pytorch_cifar10 -c 3 /workspace/cifar10_images/cat.png
				/workspace/install/bin/image_client -m pytorch_cifar10 -c 3 /workspace/cifar10_images/deer.png
				/workspace/install/bin/image_client -m pytorch_cifar10 -c 3 /workspace/cifar10_images/truck.png

[-] Usage (Kubernetes).
	https://github.com/triton-inference-server/server/tree/main/deploy
	https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/

	- (Common) enable NVIDIA device plugin for Kubernetes:
		https://github.com/NVIDIA/k8s-device-plugin

		Preparing your GPU Nodes:
			sudo apt install nvidia-docker2
			sudo systemctl restart docker

		Enable the nvidia runtime as your default runtime on your node:
			Append to /etc/docker/daemon.json:
				{
				  ...
				  "default-runtime": "nvidia",
				  "runtimes": {
					"nvidia": {
					  "path": "/usr/bin/nvidia-container-runtime",
					  "runtimeArgs": []
					}
				  }
				}

			sudo systemctl restart docker

			sudo systemctl status docker
			journalctl -xeu docker

		Enable GPU Support in Kubernetes:
			$ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.11.0/nvidia-device-plugin.yml

	- KServe.
		Refer to kserve_usage_guide.txt

	- Seldon.
		Refer to seldon_usage_guide.txt

	- Kubernetes only.
		kubectl apply -n model-serving -f triton_kube_example.yaml
			${HOME}/work/testbed/kube/triton/triton_kube_example.yaml
			Not yet completed.

	- Kubernetes + Helm.
		https://github.com/triton-inference-server/server/tree/main/deploy/k8s-onprem

		Create a namespace.
			Set up a namespace of 'model-serving':
				kubectl create namespace model-serving

			(Optional) set the current workspace to use the 'model-serving' namespace so all our commands are run there by default (instead of running everything in the default namespace):
				kubectl config set-context $(kubectl config current-context) --namespace=model-serving

		Create PV and PVC:
			kubectl apply -n model-serving -f ml_model_pv.yaml

		Copy models:
			In different terminal, copy models from local into PV:
				kubectl exec -it <PV_POD_NAME> -- bash
					kubectl exec -n model-serving -it ml-model-pv-pod -- bash

			kubectl cp <MODEL_REPOSITORY>/<MODEL_DIR> <PV_POD_NAME>:<PV_MOUNT_PATH> -c <PV_CONTAINER_NAME> -n <NAMESPACE_NAME>
				kubectl cp ${SWDT_PYTHON_HOME}/rnd/doc/machine_learning/triton/model_repository/pytorch_cifar10 ml-model-pv-pod:/mnt/ml-model-repo -c ml-model-pv-container -n model-serving 
				kubectl cp ${TRITON_INFERENCE_SERVER_HOME}/docs/examples/model_repository/simple ml-model-pv-pod:/mnt/ml-model-repo -c ml-model-pv-container -n model-serving 
				kubectl cp ${TRITON_INFERENCE_SERVER_HOME}/docs/examples/model_repository/densenet_onnx ml-model-pv-pod:/mnt/ml-model-repo -c ml-model-pv-container -n model-serving 
					https://github.com/triton-inference-server/server/tree/main/docs/examples/model_repository

		Deploy Prometheus and Grafana:
			helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
			helm repo update
			helm install triton-example-metrics -n model-serving --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false prometheus-community/kube-prometheus-stack

		Enable Autoscaling:
			To enable autoscaling, ensure that autoscaling tag in values.yamlis set to true.
		Enable Load Balancing:
			To enable load balancing, ensure that the loadBalancing tag in values.yaml is set to true.

		Deploy Triton with default settings:
			helm dependency update ./k8s-onprem -n model-serving

			helm install triton-example ./k8s-onprem -n model-serving
			helm install triton-example ./k8s-onprem -n model-serving --set autoscaler.minReplicas=2
			cat << EOF > config.yaml
			#namespace: model-serving
			image:
			  imageName: nvcr.io/nvidia/tritonserver:21.07-py3
			  pullPolicy: IfNotPresent
			  modelPersistentVolumeClaim: ml-model-pv-claim
			  numGpus: 1
			EOF
			helm install -f config.yaml triton-example ./k8s-onprem -n model-serving

			kubectl get deploy -n model-serving -o wide
			kubectl get po -n model-serving -o wide

			kubectl exec -n model-serving -c triton-inference-server -it <POD_NAME> -- bash

		Use Triton Inference Server:
			kubectl get services -n model-serving -o wide

			export CLUSTER_IP=`kubectl get svc -n model-serving -l app.kubernetes.io/name=traefik -o=jsonpath='{.items[0].spec.clusterIP}'`
			curl $CLUSTER_IP:8000/v2

			Get the client libraries:
				docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

			Run the client image:
				docker run -it --rm -v/full/path/to/model_repository:/workspace --net=host nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk
					docker run -it --rm -v/home/sangwook/work/SWDT_github/sw_dev/python/rnd/doc/machine_learning/triton/cifar10_images:/workspace/cifar10_images --net=host nvcr.io/nvidia/tritonserver:21.07-py3-sdk

				export CLUSTER_IP=<CLUSTER_IP>

				Perform inferencing using image classification models on the inference server:
					/workspace/install/bin/image_client -u $CLUSTER_IP:8000 -m pytorch_cifar10 -c 3 /workspace/cifar10_images/cat.png
					/workspace/install/bin/image_client -u $CLUSTER_IP:8000 -m pytorch_cifar10 -c 3 /workspace/cifar10_images/deer.png
					/workspace/install/bin/image_client -u $CLUSTER_IP:8000 -m pytorch_cifar10 -c 3 /workspace/cifar10_images/truck.png

				Test Load Balancing and Autoscaling:
					/workspace/install/bin/perf_analyzer -m simple -u $CLUSTER_IP:8000 --concurrency-range 1:10

		Cleanup:
			helm list -n model-serving

			helm delete triton-example -n model-serving
			helm delete triton-example-metrics -n model-serving

			For the Prometheus and Grafana services:
				kubectl delete -n model-serving crd alertmanagers.monitoring.coreos.com servicemonitors.monitoring.coreos.com podmonitors.monitoring.coreos.com prometheuses.monitoring.coreos.com prometheusrules.monitoring.coreos.com

[-] Installation.
	https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md

	- Install Docker.

	- Install NVIDIA Container Toolkit.
		https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
		https://github.com/NVIDIA/nvidia-docker

		distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
		sudo apt update
		sudo apt install nvidia-container-toolkit2

		sudo systemctl restart docker

	- Install Triton Inference Server.
		https://ngc.nvidia.com/

		docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3

	- Install Triton client.
		https://github.com/triton-inference-server/client

		Docker:
			docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		pip:
			pip install nvidia-pyindex
			pip install tritonclient[all]
			pip install tritonclient[http]

		Github:
		CMake:
