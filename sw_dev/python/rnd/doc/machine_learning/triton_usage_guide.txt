[-] General.
	- Site.
		https://developer.nvidia.com/nvidia-triton-inference-server
		https://github.com/triton-inference-server/server
		https://github.com/triton-inference-server/client

	- Document.
		https://github.com/triton-inference-server/server/blob/main/README.md#documentation

		Metrics:
			https://github.com/triton-inference-server/server/blob/main/docs/metrics.md

[-] Usage.
	- Quickstart.
		https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md

		Create a model repository:
			cd ${TritonInferenceServer_HOME}/docs/examples
			./fetch_models.sh

		Run Triton:
			docker run --gpus all --rm nvcr.io/nvidia/tritonserver:<xx.yy>-py3 nvidia-smi
				For CPU-only system:
					<error> WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
			docker run --rm nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --help

			Run on system with GPUs:
				docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
					docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/sangwook/my_repo/cpp/triton_inference_server_github/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:21.07-py3 tritonserver --model-repository=/models
				docker run --gpus=1 --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models --http-port=8000 --grpc-port=8001 --metrics-port=8002 --log-verbose=1
			Run on CPU-only system:
				docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
				docker run --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models --http-port=8000 --grpc-port=8001 --metrics-port=8002 --log-verbose=1

		Verify Triton is running correctly:
			curl -v localhost:8000/v2/health/ready

		Get the client libraries:
			docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		Run the client image:
			docker run -it --rm --net=host nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		Run the simple examples:
			/workspace/install/bin/simple_http_infer_client
			/workspace/install/bin/simple_http_async_infer_client
			/workspace/install/bin/simple_http_shm_client
			/workspace/install/bin/simple_http_cudashm_client

			/workspace/install/bin/simple_grpc_infer_client
			/workspace/install/bin/simple_grpc_async_infer_client
			/workspace/install/bin/simple_grpc_shm_client
			/workspace/install/bin/simple_grpc_cudashm_client

			/workspace/install/bin/simple_http_string_infer_client
			/workspace/install/bin/simple_grpc_string_infer_client

			/workspace/install/bin/simple_http_sequence_sync_infer_client
			/workspace/install/bin/simple_grpc_sequence_stream_infer_client
			/workspace/install/bin/simple_grpc_sequence_sync_infer_client

			/workspace/install/bin/simple_http_model_control
			/workspace/install/bin/simple_grpc_model_control

			/workspace/install/bin/simple_http_health_metadata
			/workspace/install/bin/simple_grpc_health_metadata

		Run the image classification example:
			/workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION /workspace/images/mug.jpg
			/workspace/install/bin/image_client -m densenet_onnx -c 3 -s VGG /workspace/images/mug.jpg

			/workspace/install/bin/image_client -m inception_graphdef -c 3 -s INCEPTION /workspace/images/mug.jpg
			/workspace/install/bin/image_client -m inception_graphdef -c 3 -s VGG /workspace/images/mug.jpg

		Analyze performance:
			/workspace/install/bin/perf_analyzer -m simple
			/workspace/install/bin/perf_analyzer -m simple_string
			/workspace/install/bin/perf_analyzer -m simple_sequence
			/workspace/install/bin/perf_analyzer -m densenet_onnx

	- Deployment.
		https://github.com/triton-inference-server/server/tree/main/deploy

	- TorchScript.
		REF [directory] >> ./triton

		Prepare models:
			cd ./triton

			Train models:
				python cifar10_trainer.py

			For GPU:
				cp ./cifar10_gpu_ts.pth ./model_repository/pytorch_cifar10/1/model.pt
			For CPU:
				cp ./cifar10_cpu_ts.pth ./model_repository/pytorch_cifar10/1/model.pt

		Run Triton:
			Run on system with GPUs:
				docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
					docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/sangwook/work/SWDT_github/sw_dev/python/rnd/doc/machine_learning/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.07-py3 tritonserver --model-repository=/models

			Run on CPU-only system:
				docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
					docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/sangwook/work/SWDT_github/sw_dev/python/rnd/doc/machine_learning/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.07-py3 tritonserver --model-repository=/models

		Verify Triton is running correctly:
			curl -v localhost:8000/v2/health/ready

		Get the client libraries:
			docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		Run the client image:
			docker run -it --rm -v/full/path/to/model_repository:/workspace --net=host nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk
				docker run -it --rm -v/home/sangwook/work/SWDT_github/sw_dev/python/rnd/doc/machine_learning/triton/cifar10_images:/workspace/cifar10_images --net=host nvcr.io/nvidia/tritonserver:21.07-py3-sdk

		Run the image classification example:
			/workspace/install/bin/image_client -m pytorch_cifar10 -c 3 /workspace/cifar10_images/cat.png
			/workspace/install/bin/image_client -m pytorch_cifar10 -c 3 /workspace/cifar10_images/deer.png
			/workspace/install/bin/image_client -m pytorch_cifar10 -c 3 /workspace/cifar10_images/truck.png

[-] Installation.
	https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md

	- Install Docker.

	- Install NVIDIA Container Toolkit.
		https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
		https://github.com/NVIDIA/nvidia-docker

		distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
		sudo apt update
		sudo apt install nvidia-container-toolkit2

		sudo systemctl restart docker

	- Install Triton Inference Server.
		https://ngc.nvidia.com/

		docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3

	- Install Triton client.
		https://github.com/triton-inference-server/client

		Docker:
			docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk

		pip:
			pip install nvidia-pyindex
			pip install tritonclient[all]
			pip install tritonclient[http]

		Github:
		CMake:
