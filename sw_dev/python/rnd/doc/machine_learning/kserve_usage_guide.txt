[-] General.
	- Site.
		https://github.com/kserve/kserve
		https://github.com/kubeflow/kfserving

		https://www.kubeflow.org/docs/components/kfserving/

	- API.
		https://kserve.github.io/website/reference/api/
		https://github.com/kserve/website/blob/main/docs/reference/api.md

[-] Tutorial.
	- First InferenceService.
		https://kserve.github.io/website/get_started/first_isvc/

		Create test InferenceService:
			kubectl create namespace kserve-test
			kubectl apply -f sklearn.yaml -n kserve-test

		Check InferenceService status:
			kubectl get inferenceservices sklearn-iris -n kserve-test

		Determine the ingress IP and ports:
			kubectl get svc istio-ingressgateway -n istio-system

			Load Balancer:
				If the EXTERNAL-IP value is set, your environment has an external load balancer that you can use for the ingress gateway.

				export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
				export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
			Node Port:
				If the EXTERNAL-IP value is none (or perpetually pending), your environment does not provide an external load balancer for the ingress gateway.
				In this case, you can access the gateway using the service's node port.

				# Google Kubernetes Engine (GKE).
				export INGRESS_HOST=worker-node-address
				# Minikube.
				export INGRESS_HOST=$(minikube ip)
				# Other environment (on premise).
				export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}')

				export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
			Port Forward:
				Alternatively you can do Port Forward for testing purpose.

				In another terminal:
					INGRESS_GATEWAY_SERVICE=$(kubectl get svc --namespace istio-system --selector="app=istio-ingressgateway" --output jsonpath='{.items[0].metadata.name}')
					kubectl port-forward --namespace istio-system svc/${INGRESS_GATEWAY_SERVICE} 8080:80

				export INGRESS_HOST=localhost
				export INGRESS_PORT=8080

		Curl the InferenceService:
			From Ingress gateway with HOST Header:
				SERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris -n kserve-test -o jsonpath='{.status.url}' | cut -d "/" -f 3)

			From outside of the kubernetes cluster:
				curl -v -H "Host: ${SERVICE_HOSTNAME}" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/sklearn-iris:predict -d @./iris-input.json
			In the kubernetes cluseter:
				curl -v -H "Host: ${SERVICE_HOSTNAME}" http://<istio-ingressgateway ClusterIP>/v1/models/sklearn-iris:predict -d @./iris-input.json

		Run performance test:
			kubectl create -f https://raw.githubusercontent.com/kserve/kserve/release-0.7/docs/samples/v1beta1/sklearn/v1/perf.yaml -n kserve-test
				Use kubectl create instead of apply because the job template is using generateName which doesn't work with kubectl apply.

	- Deploy sklearn-learn models with InferenceService.
		https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/

		Test locally:
			Pre-requisites:
				pip install mlserver mlserver-sklearn

			Model settings:
				In model-settings.json:
					{
					  "name": "sklearn-iris",
					  "version": "v1.0.0",
					  "implementation": "mlserver_sklearn.SKLearnModel"
					}

			Serve our model locally:
				mlserver start .

		Deploy with InferenceService:
			kubectl apply -f ./sklearn.yaml

		Test deployed model:
			Refer to "Determine the ingress IP and ports".

			SERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-irisv2 -o jsonpath='{.status.url}' | cut -d "/" -f 3)
			curl -v -H "Host: ${SERVICE_HOSTNAME}" -d @./iris-input.json http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/sklearn-irisv2/infer

	- Predict on an InferenceService with a saved model on PVC.
		https://kserve.github.io/website/modelserving/storage/pvc/pvc/

		Install Scikit-Learn Server:
			https://github.com/kserve/kserve/tree/master/python/sklearnserver

		Create PV and PVC:
			kubectl apply -f pv-and-pvc.yaml

		Copy model to PV:
			kubectl apply -f pv-model-store.yaml

			kubectl exec -it <PV_POD_NAME> -- bash
				kubectl exec -it model-store-pod -- bash
				kubectl exec -it ml-model-pv-pod -- bash

			In different terminal, copy the model from local into PV:
				kubectl cp model.joblib <PV_POD_NAME>:<PV_MOUNT_PATH>/model.joblib -c <PV_CONTAINER_NAME>
					kubectl cp model.joblib model-store-pod:/pv/model.joblib -c model-store
					kubectl cp model.joblib ml-model-pv-pod:/ml-model-pv/model.joblib -c ml-model-pv-container

		Deploy InferenceService with models on PVC:
			kubectl apply -f sklearn-pvc.yaml

		Run a prediction:
			SERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-pvc -o jsonpath='{.status.url}' | cut -d "/" -f 3)

			MODEL_NAME=sklearn-pvc
			INPUT_PATH=@./input.json
			curl -v -H "Host: ${SERVICE_HOSTNAME}" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH

	- Deploy Tensorflow Model with InferenceService.
		https://kserve.github.io/website/modelserving/v1beta1/tensorflow/
		https://github.com/kserve/kserve/tree/master/docs/samples/v1beta1/tensorflow

		Create the HTTP InferenceService:
			Apply the tensorflow.yaml to create the InferenceService, by default it exposes a HTTP/REST endpoint:
				kubectl apply -f tensorflow.yaml 
				kubectl get isvc flower-sample

			Run a prediction:
				Refer to "Determine the ingress IP and ports".

				MODEL_NAME=flower-sample
				INPUT_PATH=@./input.json
				SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d "/" -f 3)
				curl -v -H "Host: ${SERVICE_HOSTNAME}" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH

		Canary Rollout:
			Canary rollout is a great way to control the risk of rolling out a new model by first moving a small percent of the traffic to it and then gradually increase the percentage.

			Run a canary rollout (canary.yaml with the canaryTrafficPercent field specified):
				kubectl apply -f canary.yaml

			Verify if the traffic split percentage is applied correctly:
				kubectl get isvc flower-example

			Run a prediction:
				MODEL_NAME=flower-example
				INPUT_PATH=@./input.json
				SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d "/" -f 3)
				curl -v -H "Host: ${SERVICE_HOSTNAME}" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH

		Create the gRPC InferenceService:
			Apply grpc.yaml to create the gRPC InferenceService:
				kubectl apply -f grpc.yaml 

				pip install tensorflow-serving-api>=1.14.0,<2.0.0

			Run a prediction:
				MODEL_NAME=flower-grpc
				INPUT_PATH=@./input.json
				SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d "/" -f 3)
				python grpc_client.py --host $INGRESS_HOST --port $INGRESS_PORT --model $MODEL_NAME --hostname $SERVICE_HOSTNAME --input_path $INPUT_PATH

	- Deploy PyTorch model with TorchServe InferenceService.
		https://kserve.github.io/website/modelserving/v1beta1/torchserve/

		(Optional) create model storage with model archive file:
			Generate model archiver files for torchserve.
				https://kserve.github.io/website/modelserving/v1beta1/torchserve/model-archiver/

			(Optional) install Torchserve Model Archive Files (MAR):
				https://github.com/pytorch/serve

				pip install model-archiver
				conda install torch-model-archiver -c pytorch

			torch-model-archiver --model-name mnist --version 1.0 \
			--model-file model-archiver/model-store/mnist/mnist.py \
			--serialized-file model-archiver/model-store/mnist/mnist_cnn.pt \
			--handler model-archiver/model-store/mnist/mnist_handler.py
				https://github.com/pytorch/serve/tree/master/examples/image_classifier/mnist
				Creates mnist.mar in the current directory.

		Create the InferenceService:
			For deploying the InferenceService on CPU:
				kubectl apply -f torchserve.yaml -n kserve-test
			For deploying the InferenceService on GPU:
				kubectl apply -f gpu.yaml -n kserve-test

		Inference:
			Refer to "Determine the ingress IP and ports".

			MODEL_NAME=mnist
			SERVICE_HOSTNAME=$(kubectl get inferenceservice torchserve -n kserve-test -o jsonpath='{.status.url}' | cut -d "/" -f 3)

			curl -v -H "Host: ${SERVICE_HOSTNAME}" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d @./mnist.json

		Explanation:
			curl -v -H "Host: ${SERVICE_HOSTNAME}" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/mnist:explain -d @./mnist.json

		Autoscaling:
			https://kserve.github.io/website/modelserving/v1beta1/torchserve/autoscaling/

		Canary Rollout:
			https://kserve.github.io/website/modelserving/v1beta1/torchserve/canary/

		Monitoring:
			https://kserve.github.io/website/modelserving/v1beta1/torchserve/metrics/

	- Predict on a Triton InferenceService with TorchScript model (PyTorch).
		https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/

	- QA Inference with BERT model using Triton Inference Server (TensorFlow).
		https://kserve.github.io/website/modelserving/v1beta1/triton/bert/

	- Deploy Custom Python Model Server with InferenceService.
		https://kserve.github.io/website/modelserving/v1beta1/custom/custom_model/

[-] Usage.
	- AWS EBS.
		https://mokpolar.github.io/kfserving_custum_mobilenet/

[-] Installation.
	https://kserve.github.io/website/get_started/
	https://raw.githubusercontent.com/kserve/kserve/release-0.7/hack/quick_install.sh

	- Serverless mode.
		https://kserve.github.io/website/0.8/admin/serverless/
		https://kserve.github.io/website/admin/serverless/

		Method 1:
			Install Istio:
				https://istio.io/latest/docs/setup/install/istioctl/

			Install Knative:
				https://knative.dev/docs/install/yaml-install/serving/install-serving-with-yaml/

			Install Cert Manager:
				https://cert-manager.io/docs/installation/

		Method 2:
			https://knative.dev/docs/install/serving/install-serving-with-yaml/

			Install Knative:
				kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.2.0/serving-crds.yaml
				kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.2.0/serving-core.yaml
					Edit serving-core.yaml:
						apiVersion: autoscaling/v2beta2 -> apiVersion: autoscaling/v2

				kubectl get pods -n knative-serving

			Install Istio:
				kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.2.0/istio.yaml
				kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.2.0/istio.yaml
				kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.2.0/net-istio.yaml

				kubectl get service istio-ingressgateway -n istio-system
				kubectl get pods -n istio-system
				kubectl get pods -n knative-serving

			Install Cert Manager:
				kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.yaml
					Needs at least one node to run. (?)
				kubectl apply -f https://github.com/knative/net-certmanager/releases/download/knative-v1.2.0/release.yaml

				kubectl delete apiservice v1beta1.webhook.cert-manager.io
				kubectl delete namespace cert-manager

				kubectl get pods --namespace cert-manager
				kubectl get apiservice --all-namespaces

		Install KServe:
			kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.8.0/kserve.yaml
			kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.8.0/kserve-runtimes.yaml

		Check:
			kubectl get ksvc --all-namespaces
			kubectl get isvc --all-namespaces

	- Raw deployment mode.
		https://kserve.github.io/website/admin/kubernetes_deployment/

[-] Troubleshooting.
	- <error>
		No ServingRuntimes or ClusterServingRuntimes with the name: kserve-mlserver
		No ServingRuntimes or ClusterServingRuntimes with the name: kserve-sklearnserver
		<check>
			kubectl describe isvc <INFERENCESERVICE_NAME>
		<cause>
			ClusterServingRuntimes were not installed.
		<solution>
			kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.8.0/kserve-runtimes.yaml

			Download kserve-runtimes.yaml from https://github.com/kserve/kserve/releases.
			kubectl apply -f kserve-runtimes.yaml

	- <error>
		Unable to fetch image "kserve/sklearnserver:v0.8.0": failed to resolve image to digest: Get "https://index.docker.io/v2/": context deadline exceeded.
		Unable to fetch image "docker.io/seldonio/mlserver:0.5.3": failed to resolve image to digest: Get "https://index.docker.io/v2/": context deadline exceeded.
		<check>
			kubectl get ksvc --all-namespaces
			kubectl get isvc --all-namespaces
			kubectl describe ksvc <KSERVICE_NAME>
			kubectl describe isvc <INFERENCESERVICE_NAME>
			kubectl get events
		<solution>
			docker search kserve/sklearnserver
			docker search seldonio/mlserver

			Search for "kserve/sklearnserver" & "seldonio/mlserver" in https://hub.docker.com/.

			https://github.com/kserve/kserve/tree/master/python/sklearnserver

			Download kserve.yaml & kserve-runtimes.yaml from https://github.com/kserve/kserve/releases.
			Search for "sklearnserver" & "mlserver" in kserve.yaml & kserve-runtimes.yaml.

			https://kangwoo.kr/2020/04/25/seldon-core-sklearn-server/
				컨테이너 이미지를 빌드.
				컨테이너 이미지를 레지스트리에 푸시.
				쿠버네티스 잡(Job)을 생성.
				Job을 생성할 때는 모델을 저장하기 위해서 PVC를 마운트.
			https://ikcoo.tistory.com/65
			https://morian-kim.tistory.com/35
			https://www.kubeflow.org/docs/external-add-ons/kserve/first_isvc_kserve/
