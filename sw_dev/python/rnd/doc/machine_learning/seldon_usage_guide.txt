[-] General.
	- Site.
		https://www.seldon.io/
		https://github.com/SeldonIO/seldon-core/

	- Document.
		https://docs.seldon.io/
		https://docs.seldon.io/projects/seldon-core/en/latest/

		API:
		https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/index.html

[-] Tutorial.
	- Prepackaged Model Servers:
		https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html

	- Quickstart.
		https://docs.seldon.io/projects/seldon-core/en/latest/workflow/quickstart.html
		https://docs.seldon.io/projects/seldon-core/en/latest/workflow/github-readme.html

		Install s2i.
			https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/s2i.html
			https://github.com/openshift/source-to-image

		Export model binaries and/or artifacts:
			https://scikit-learn.org/stable/modules/model_persistence.html

		Wrap the model with our language wrappers:
			Create a wrapper class:
				In Model.py:
					import pickle, joblib

					class Model:
						def __init__(self):
							#self._model = pickle.load(open("./model.pickle", "rb"))
							self._model = joblib.load("./model.joblib")

						def predict(self, X, *args):
							output = self._model.predict(X.reshape(1, -1))
							return output

			(Optional) test model locally:
				In a terminal:
					seldon-core-microservice Model --service-type MODEL

				In another terminal:
					curl -X POST localhost:9000/api/v1.0/predictions \
						-H 'Content-Type: application/json' \
						-d '{ "data": { "ndarray": [1,2,3,4] } }' \
						| json_pp

			Use the Seldon tools to containerise the model:
				https://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping_s2i.html

				s2i usage seldonio/seldon-core-s2i-python3:0.18

				Edit requirements.txt: (not working)
					scikit-learn==0.19.1
				Edit setup.py:
					from setuptools import setup

					setup(
					  name="my-model",
					  # ...
					  install_requires=[
						"scikit-learn",
					  ]
					)
				Edit environment.yml: (not working)
					name: <CONDA_ENV_NAME>
					channels:
					  - defaults
					dependencies:
					  - python=3.6
					  - scikit-learn=0.19.1

				s2i build . seldonio/seldon-core-s2i-python3:0.18 sklearn_iris:0.1 --env MODEL_NAME=Model --env SERVICE_TYPE=MODEL --env API_TYPE=REST --env PERSISTENCE=0
				s2i build . seldonio/seldon-core-s2i-python3:0.18 sklearn_iris:0.1 --env MODEL_NAME=Model --env SERVICE_TYPE=MODEL --env API_TYPE=REST --env PERSISTENCE=0 --env CONDA_ENV_NAME=<CONDA_ENV_NAME>

				docker run -p 5000:5000 sklearn_iris:0.1
				docker run -it -p 5000:5000 sklearn_iris:0.1 /bin/bash

			Deploy to Kubernetes:
				https://docs.seldon.io/projects/seldon-core/en/latest/graph/inference-graph.html

				kubectl create namespace model-namespace

				kubectl apply -f - << END
				apiVersion: machinelearning.seldon.io/v1
				kind: SeldonDeployment
				metadata:
				  name: iris-model
				  namespace: model-namespace
				spec:
				  name: iris
				  predictors:
				  - componentSpecs:
					- spec:
						containers:
						- name: classifier
						  image: sklearn_iris:0.1
					graph:
					  name: classifier
					name: default
					replicas: 1
				END

			Send a request to your deployed model in Kubernetes:
				Refer to istio_usage_guide.txt to determine the ingress IP and ports.

				Send requests directly through a browser:
					http://<ingress_url>/seldon/<namespace>/<model-name>/api/v1.0/doc/

				Send requests programmatically using Seldon Python Client:
					https://docs.seldon.io/projects/seldon-core/en/latest/python/seldon_client.html

				Send requests programmatically using Linux CLI:
					curl -X POST http://<ingress>/seldon/model-namespace/iris-model/api/v1.0/predictions \
						-H 'Content-Type: application/json' \
						-d '{ "data": { "ndarray": [1,2,3,4] } }' | json_pp

		Wrap your model with our pre-packaged inference servers:
			Upload your model to an object store:
				gsutil cp model.joblib gs://seldon-models/v1.14.0-dev/sklearn/iris/model.joblib

			Deploy to Seldon Core in Kubernetes:
				kubectl apply -f - << END
				apiVersion: machinelearning.seldon.io/v1
				kind: SeldonDeployment
				metadata:
				  name: iris-model
				  namespace: model-namespace
				spec:
				  name: iris
				  predictors:
				  - graph:
					  implementation: SKLEARN_SERVER
					  modelUri: gs://seldon-models/v1.14.0-dev/sklearn/iris
					  name: classifier
					name: default
					replicas: 1
				END

			Send a request in Kubernetes cluster:
				Refer to istio_usage_guide.txt to determine the ingress IP and ports.

				Send requests directly through a browser:
					http://<ingress_url>/seldon/<namespace>/<model-name>/api/v1.0/doc/

				Send requests programmatically using Seldon Python Client:
					https://docs.seldon.io/projects/seldon-core/en/latest/python/seldon_client.html

				Send requests programmatically using Linux CLI:
					curl -X POST http://<ingress>/seldon/model-namespace/iris-model/api/v1.0/predictions \
						-H 'Content-Type: application/json' \
						-d '{ "data": { "ndarray": [1,2,3,4] } }' | json_pp

[-] Installation.
	https://docs.seldon.io/projects/seldon-core/en/latest/nav/installation.html
	https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html

	- Install.
		kubectl label namespace default istio-injection=enabled

		Create Istio Gateway:
			https://docs.seldon.io/projects/seldon-core/en/latest/ingress/istio.html

			kubectl apply -f - << END
			apiVersion: networking.istio.io/v1alpha3
			kind: Gateway
			metadata:
			  name: seldon-gateway
			  namespace: istio-system
			spec:
			  selector:
				istio: ingressgateway # use istio default controller
			  servers:
			  - port:
				  number: 80
				  name: http
				  protocol: HTTP
				hosts:
				- "*"
			END

		Install Seldon Core:
			kubectl create namespace seldon-system

			helm install seldon-core seldon-core-operator \
				--repo https://storage.googleapis.com/seldon-charts \
				--set usageMetrics.enabled=true \
				--set istio.enabled=true \
				--namespace seldon-system
			helm install seldon-core seldon-core-operator \
				--repo https://storage.googleapis.com/seldon-charts \
				--set usageMetrics.enabled=true \
				--set istio.enabled=true \
				--set certManager.enabled=true \
				--namespace seldon-system

			helm install seldon-core seldon-core-operator \
				--repo https://storage.googleapis.com/seldon-charts \
				--set usageMetrics.enabled=true \
				--set istio.enabled=true \
				--set istio.gateway=<GATEWAY_NAME> \
				--namespace seldon-system

			Check that your Seldon Controller is running by doing:
				kubectl get pods -n seldon-system
					You should see a seldon-controller-manager pod with STATUS=Running.

		Local Port Forwarding:
			Because your kubernetes cluster is running locally, we need to forward a port on your local machine to one in the cluster for us to be able to access it externally.

			kubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80
				Listen on port 8080 locally, forwarding to the service's port 80.

[-] Troubelshooting.
	- <error>
		a container name must be specified for pod iris-model-default-0-classifier-6b877bdb78-sbrkq, choose one of: [classifier seldon-container-engine] or one of the init containers: [classifier-model-initializer]
		a container name must be specified for pod iris-model-default-0-classifier-86cc4bb9cc-7x48h, choose one of: [classifier seldon-container-engine]
		<check>
			kubectl get deploy --all-namespaces -o wide
			kubectl get svc --all-namespaces -o wide
			kubectl get pods --all-namespaces -o wide
			kubectl logs --all-namespaces -o wide
			kubectl describe pod <POD_NAME> -n <NAMESPACE_NAME>
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME>
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c classifier
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c seldon-container-engine
		<cause> Error when containerising the model by seldonio/seldon-core-s2i-python3
		<solution>
			Refer to "Error when containerising the model by seldonio/seldon-core-s2i-python3".

	- <error> Failed to connect to 192.168.20.11 port 32591: Connection refused
		<cause> No Istio gateway.
		<solution>
			https://docs.seldon.io/projects/seldon-core/en/latest/ingress/istio.html

			You need an istio gateway installed in the istio-system namespace.
