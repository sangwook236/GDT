[-] General.
	- Site.
		https://www.seldon.io/
		https://github.com/SeldonIO/seldon-core/

	- Document.
		https://docs.seldon.io/
		https://docs.seldon.io/projects/seldon-core/en/latest/

		Protocal:
			https://docs.seldon.io/projects/seldon-core/en/latest/graph/svcorch.html
			https://docs.seldon.io/projects/seldon-core/en/latest/examples/protocol_examples.html

	- API.
		https://docs.seldon.io/projects/seldon-core/en/latest/reference/seldon-deployment.html
		https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/index.html

[-] Usage.
	https://deploy.seldon.io/en/v1.2/contents/demos/seldon-core/index.html

	- Make models available directly from PVCs.
		Export model binaries and/or artifacts:
			https://github.com/SeldonIO/seldon-core/blob/master/servers/sklearnserver/models/iris/train.py
			https://scikit-learn.org/stable/modules/model_persistence.html

			The expected versions in the latest SKLearn pre-packaged server:
				https://docs.seldon.io/projects/seldon-core/en/latest/servers/sklearn.html

		Create PV and PVC:
			kubectl create namespace model-serving

			kubectl apply -n model-serving -f - << END
			apiVersion: v1
			kind: PersistentVolume
			metadata:
			  name: ml-model-pv-volume
			  labels:
				type: local
			spec:
			  storageClassName: manual
			  capacity:
				storage: 2Gi
			  accessModes:
				- ReadWriteOnce
			  hostPath:
				path: "/mnt/ml-model-pv"
			---
			apiVersion: v1
			kind: PersistentVolumeClaim
			metadata:
			  name: ml-model-pv-claim
			spec:
			  storageClassName: manual
			  accessModes:
				- ReadWriteOnce
			  resources:
				requests:
				  storage: 1Gi
			---
			apiVersion: v1
			kind: Pod
			metadata:
			  name: ml-model-pv-pod
			spec:
			  volumes:
				- name: ml-model-pv-storage
				  persistentVolumeClaim:
					claimName: ml-model-pv-claim
			  containers:
				- name: ml-model-pv-container
				  image: ubuntu
				  command: [ "sleep" ]
				  args: [ "infinity" ]
				  volumeMounts:
					- mountPath: "/mnt/ml-model-pv"
					  name: ml-model-pv-storage
					  readOnly: false
				  resources:
					limits:
					  memory: "1Gi"
					  cpu: "1"
			END

		Copy the model file:
			kubectl exec -it <PV_POD_NAME> -- bash
				kubectl exec -n model-serving -it ml-model-pv-pod -- bash

			In different terminal, copy the model from local into PV:
				kubectl cp model.joblib <PV_POD_NAME>:<PV_MOUNT_PATH>/model.joblib -c <PV_CONTAINER_NAME> -n model-serving 
					kubectl cp model.joblib ml-model-pv-pod:/mnt/ml-model-pv/model.joblib -c ml-model-pv-container -n model-serving 

		Deploy to Seldon Core in Kubernetes:
			https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html
			https://docs.seldon.io/projects/seldon-core/en/latest/servers/sklearn.html
			https://github.com/SeldonIO/seldon-core/tree/master/servers/sklearnserver

			kubectl apply -n model-serving -f - << END
			apiVersion: machinelearning.seldon.io/v1
			kind: SeldonDeployment
			metadata:
			  name: sklearn-iris
			spec:
			  name: iris
			  predictors:
			  - name: default
				replicas: 1
				graph:
				  #children: []
				  name: classifier
				  type: MODEL
				  modelUri: pvc://ml-model-pv-claim/mnt/ml-model-pv
				  #parameters:  # Not working.
				  #- name: method
				  #  type: STRING
				  #  value: predict
				  #- name: model_uri
				  #  type: STRING
				  #  value: /mnt/models
				componentSpecs:
				- spec:
					volumes:
					- name: classifier-provision-location
					  persistentVolumeClaim:
						claimName: ml-model-pv-claim
					containers:
					- name: classifier
					  image: seldonio/sklearnserver:1.14.0-dev
					  imagePullPolicy: IfNotPresent
					  volumeMounts:
					  - mountPath: /mnt/models
						name: classifier-provision-location
						readOnly: true
					  env:
					  - name: PREDICTIVE_UNIT_PARAMETERS
						#value: '[{"name": "model_uri", "value": "/mnt/models", "type": "STRING"}]'
						value: '[{"name": "model_uri", "value": "/mnt/models", "type": "STRING"}, {"name": "method", "value": "predict", "type": "STRING"}]'
			END

			kubectl get deploy --all-namespaces -o wide
			kubectl get svc --all-namespaces -o wide
			kubectl get pods --all-namespaces -o wide
			kubectl logs --all-namespaces -o wide
			kubectl describe pod <POD_NAME> -n <NAMESPACE_NAME>
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME>
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c classifier
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c seldon-container-engine
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c istio-proxy
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c classifier-model-initializer
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c istio-init

			kubectl exec -n <NAMESPACE_NAME> -c classifier -it <POD_NAME> -- bash

		Send a request in Kubernetes cluster:
			Refer to istio_usage_guide.txt to determine the ingress IP and ports.

			Send requests directly through a browser:
				http://<INGRESS_URL>/seldon/<NAMESPACE_NAME>/<MODEL_NAME>/api/v1.0/doc/
					http://192.168.20.11:32591/seldon/model-serving/sklearn-iris/api/v1.0/doc/

			Send requests programmatically using Seldon Python Client:
				https://docs.seldon.io/projects/seldon-core/en/latest/python/seldon_client.html

			Send requests programmatically using Linux CLI:
				curl -X POST http://<INGRESS_URL>/seldon/<NAMESPACE_NAME>/<MODEL_NAME>/api/v1.0/predictions \
					-H 'Content-Type: application/json' \
					-d '{ "data": { "tensor": { "shape": [3, 4], "values": [5.1, 3.5, 1.4, 0.2, 7.0, 3.2, 4.7, 1.4, 6.3, 3.3, 6.0, 2.5] } } }' \
					| json_pp

[-] Usage (Triton Inference Server).
	https://docs.seldon.io/projects/seldon-core/en/latest/servers/triton.html
	https://docs.seldon.io/projects/seldon-core/en/latest/examples/triton_examples.html
	https://github.com/triton-inference-server/server/tree/main/deploy/k8s-onprem

	https://github.com/SeldonIO/seldon-core/blob/master/components/drift-detection/nvidia-triton-cifar10/cifar10_drift.ipynb
	https://github.com/SeldonIO/seldon-core/blob/master/components/outlier-detection/nvidia-triton-cifar10/cifar10_outlier.ipynb

	- TorchScript.
		Refer to "Usage (Kubernetes)" in triton_usage_guide.txt.

		Setup Seldon Core.
			Port-forward to that ingress on localhost:8003 in a separate terminal:
				kubectl port-forward $(kubectl get pods -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}') -n istio-system 8003:8080

		Create a namespace.
			set up a namespace of 'model-serving':
				kubectl create namespace model-serving

			(Optional) set the current workspace to use the 'model-serving' namespace so all our commands are run there by default (instead of running everything in the default namespace):
				kubectl config set-context $(kubectl config current-context) --namespace=model-serving

		Create PV and PVC:
			kubectl apply -n model-serving -f ml-model-pv.yaml

		Copy models:
			In different terminal, copy models from local into PV:
				kubectl exec -it <PV_POD_NAME> -- bash
					kubectl exec -n model-serving -it ml-model-pv-pod -- bash

			kubectl cp <MODEL_REPOSITORY>/<MODEL_DIR> <PV_POD_NAME>:<PV_MOUNT_PATH> -c <PV_CONTAINER_NAME> -n model-serving 
				kubectl cp ${SWDT_PYTHON_HOME}/rnd/doc/machine_learning/triton/model_repository/pytorch_cifar10 ml-model-pv-pod:/mnt/ml-model-pv -c ml-model-pv-container -n model-serving 
					Refer to triton_usage_guide.txt
				kubectl cp ${TRITON_INFERENCE_SERVER_HOME}/docs/examples/model_repository/densenet_onnx ml-model-pv-pod:/mnt/ml-model-pv -c ml-model-pv-container -n model-serving 
				kubectl cp ${TRITON_INFERENCE_SERVER_HOME}/docs/examples/model_repository/simple ml-model-pv-pod:/mnt/ml-model-pv -c ml-model-pv-container -n model-serving 
					https://github.com/triton-inference-server/server/tree/main/docs/examples/model_repository

		Setup resources: (???)
			https://github.com/SeldonIO/seldon-core/blob/master/components/drift-detection/nvidia-triton-cifar10/cifar10_drift.ipynb

			kubectl apply -f seldon-gateway.yaml
			kubectl apply -n model-serving -f broker.yaml
			kubectl apply -n model-serving -f event-display.yaml
			kubectl apply -f trigger.yaml

		Deploy to Seldon Core in Kubernetes:
			kubectl apply -n model-serving -f triton-seldom-example.yaml
				<error> Not working.
				<cause> A model repository on the PVC is not mounted on the docker image of Triton Inference Server.

			kubectl get po -n model-serving
			kubectl exec -n model-serving -c classifier -it <POD_NAME> -- bash

		Send a request in Kubernetes cluster:
			python triton_send_request.py

[-] Quickstart.
	https://docs.seldon.io/projects/seldon-core/en/latest/workflow/quickstart.html
	https://docs.seldon.io/projects/seldon-core/en/latest/workflow/github-readme.html
	https://docs.seldon.io/projects/seldon-core/en/latest/workflow/serving.html

	- Prerequisite.
		Install s2i.
			https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/s2i.html
			https://github.com/openshift/source-to-image

		Export model binaries and/or artifacts:
			https://github.com/SeldonIO/seldon-core/blob/master/servers/sklearnserver/models/iris/train.py
			https://scikit-learn.org/stable/modules/model_persistence.html

	- Wrap the model with our language wrappers.
		https://docs.seldon.io/projects/seldon-core/en/latest/python/index.html

		Create a wrapper class:
			https://docs.seldon.io/projects/seldon-core/en/latest/python/python_component.html

			In MyModel.py:
				import pickle, joblib

				class MyModel:
					def __init__(self):
						#self._model = pickle.load(open("./model.pickle", "rb"))
						self._model = joblib.load("./model.joblib")

					def predict(self, X, features_names=None):
						output = self._model.predict(X.reshape(1, -1))
						return output

		(Optional) test model locally:
			https://github.com/SeldonIO/seldon-core/blob/master/python/seldon_core/microservice.py

			In a terminal:
				seldon-core-microservice MyModel --service-type MODEL

			In another terminal:
				curl -X POST localhost:9000/api/v1.0/predictions \
					-H 'Content-Type: application/json' \
					-d '{ "data": { "ndarray": [1,2,3,4] } }' \
					| json_pp

		Use the Seldon tools to containerise the model:
			https://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping_s2i.html
			https://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping_docker.html
			https://github.com/SeldonIO/seldon-core/tree/master/components/rclone-storage-initializer
			https://github.com/SeldonIO/seldon-core/tree/master/components/storage-initializer

			Install s2i:
				s2i usage seldonio/seldon-core-s2i-python3:0.18

			Create a source code:
				Dependencies:
					Edit requirements.txt: (not working)
						scikit-learn==0.19.1
					Edit setup.py:
						from setuptools import setup

						setup(
						  name="my-model",
						  # ...
						  install_requires=[
							"scikit-learn",
						  ]
						)
					Edit environment.yml: (not working)
						name: <CONDA_ENV_NAME>
						channels:
						  - defaults
						dependencies:
						  - python=3.6
						  - scikit-learn=0.19.1

				Define the core parameters needed by our python builder image to wrap the model:
					In ./environment:
						MODEL_NAME=MyModel
						SERVICE_TYPE=MODEL

			Build an image:
				s2i build . seldonio/seldon-core-s2i-python3:0.18 sklearn_iris:0.1 --env MODEL_NAME=MyModel --env SERVICE_TYPE=MODEL --env API_TYPE=REST --env PERSISTENCE=0
				s2i build . seldonio/seldon-core-s2i-python3:0.18 sklearn_iris:0.1 --env MODEL_NAME=MyModel --env SERVICE_TYPE=MODEL --env API_TYPE=REST --env PERSISTENCE=0 --env CONDA_ENV_NAME=<CONDA_ENV_NAME>

			Check:
				docker run -p 5000:5000 sklearn_iris:0.1
				docker run -it -p 5000:5000 sklearn_iris:0.1 /bin/bash

		Deploy to Kubernetes:
			https://docs.seldon.io/projects/seldon-core/en/latest/graph/inference-graph.html
			https://docs.seldon.io/projects/seldon-core/en/latest/python/python_server.html

			kubectl create namespace model-namespace

			kubectl apply -f - << END
			apiVersion: machinelearning.seldon.io/v1
			kind: SeldonDeployment
			metadata:
			  name: iris-model
			  namespace: model-namespace
			spec:
			  name: iris
			  predictors:
			  - componentSpecs:
				- spec:
					containers:
					- name: classifier
					  image: sklearn_iris:0.1
				graph:
				  name: classifier
				name: default
				replicas: 1
			END

		Send a request to your deployed model in Kubernetes:
			Refer to istio_usage_guide.txt to determine the ingress IP and ports.

			Send requests directly through a browser:
				http://<INGRESS_URL>/seldon/<NAMESPACE_NAME>/<MODEL_NAME>/api/v1.0/doc/

			Send requests programmatically using Seldon Python Client:
				https://docs.seldon.io/projects/seldon-core/en/latest/python/seldon_client.html

			Send requests programmatically using Linux CLI:
				curl -X POST http://<INGRESS_URL>/seldon/<NAMESPACE_NAME>/<MODEL_NAME>/api/v1.0/predictions \
					-H 'Content-Type: application/json' \
					-d '{ "data": { "ndarray": [1,2,3,4] } }' \
					| json_pp

	- Wrap your model with our pre-packaged inference servers.
		Upload your model to an object store:
			gsutil cp model.joblib gs://seldon-models/v1.14.0-dev/sklearn/iris/model.joblib

		Deploy to Seldon Core in Kubernetes:
			https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html

			kubectl apply -f - << END
			apiVersion: machinelearning.seldon.io/v1
			kind: SeldonDeployment
			metadata:
			  name: iris-model
			  namespace: model-namespace
			spec:
			  name: iris
			  predictors:
			  - graph:
				  implementation: SKLEARN_SERVER
				  modelUri: gs://seldon-models/v1.14.0-dev/sklearn/iris
				  name: classifier
				name: default
				replicas: 1
			END

		Send a request in Kubernetes cluster:
			Refer to istio_usage_guide.txt to determine the ingress IP and ports.

			Send requests directly through a browser:
				http://<INGRESS_URL>/seldon/<NAMESPACE_NAME>/<MODEL_NAME>/api/v1.0/doc/

			Send requests programmatically using Seldon Python Client:
				https://docs.seldon.io/projects/seldon-core/en/latest/python/seldon_client.html

			Send requests programmatically using Linux CLI:
				curl -X POST http://<INGRESS_URL>/seldon/<NAMESPACE_NAME>/<MODEL_NAME>/api/v1.0/predictions \
					-H 'Content-Type: application/json' \
					-d '{ "data": { "ndarray": [1,2,3,4] } }' \
					| json_pp

[-] Installation.
	https://docs.seldon.io/projects/seldon-core/en/latest/nav/installation.html
	https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html

	- Install.
		kubectl label namespace default istio-injection=enabled

		Create Istio Gateway:
			https://docs.seldon.io/projects/seldon-core/en/latest/ingress/istio.html

			kubectl apply -f - << END
			apiVersion: networking.istio.io/v1alpha3
			kind: Gateway
			metadata:
			  name: seldon-gateway
			  namespace: istio-system
			spec:
			  selector:
				istio: ingressgateway # use istio default controller
			  servers:
			  - port:
				  number: 80
				  name: http
				  protocol: HTTP
				hosts:
				- "*"
			END

		Install Seldon Core:
			https://docs.seldon.io/projects/seldon-core/en/latest/reference/helm.html
			https://docs.seldon.io/projects/seldon-core/en/latest/graph/helm_charts.html
			https://github.com/SeldonIO/seldon-core/tree/master/helm-charts
			https://github.com/SeldonIO/seldon-core/tree/master/helm-charts/seldon-core-operator

			kubectl create namespace seldon-system

			helm install seldon-core seldon-core-operator \
				--repo https://storage.googleapis.com/seldon-charts \
				--set usageMetrics.enabled=true \
				--set istio.enabled=true \
				--namespace seldon-system
			helm install seldon-core seldon-core-operator \
				--repo https://storage.googleapis.com/seldon-charts \
				--set usageMetrics.enabled=true \
				--set istio.enabled=true \
				--set certManager.enabled=true \
				--namespace seldon-system

			helm install seldon-core seldon-core-operator \
				--repo https://storage.googleapis.com/seldon-charts \
				--set usageMetrics.enabled=true \
				--set istio.enabled=true \
				--set istio.gateway=<GATEWAY_NAME> \
				--namespace seldon-system

			Check that your Seldon Controller is running by doing:
				kubectl get pods -n seldon-system
					You should see a seldon-controller-manager pod with STATUS=Running.

		Local Port Forwarding:
			Because your kubernetes cluster is running locally, we need to forward a port on your local machine to one in the cluster for us to be able to access it externally.

			kubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80
				Listen on port 8080 locally, forwarding to the service's port 80.

[-] Troubelshooting.
	- <error>
		a container name must be specified for pod iris-model-default-0-classifier-6b877bdb78-sbrkq, choose one of: [classifier seldon-container-engine] or one of the init containers: [classifier-model-initializer]
		a container name must be specified for pod iris-model-default-0-classifier-86cc4bb9cc-7x48h, choose one of: [classifier seldon-container-engine]
		a container name must be specified for pod sklearn-iris-default-0-classifier-6cf75bd47-n8cdh, choose one of: [classifier seldon-container-engine istio-proxy] or one of the init containers: [classifier-model-initializer istio-init]
		<check>
			kubectl get deploy --all-namespaces -o wide
			kubectl get svc --all-namespaces -o wide
			kubectl get pods --all-namespaces -o wide
			kubectl logs --all-namespaces -o wide
			kubectl describe pod <POD_NAME> -n <NAMESPACE_NAME>
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME>
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c classifier
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c seldon-container-engine
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c istio-proxy
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c classifier-model-initializer
			kubectl logs <POD_NAME> -n <NAMESPACE_NAME> -c istio-init
		<cause> Error when containerising the model by seldonio/seldon-core-s2i-python3
		<solution>
			Refer to "Error when containerising the model by seldonio/seldon-core-s2i-python3".

	- <error> Failed to connect to 192.168.20.11 port 32591: Connection refused
		<cause> No Istio gateway.
		<solution>
			https://docs.seldon.io/projects/seldon-core/en/latest/ingress/istio.html

			You need an istio gateway installed in the istio-system namespace.
